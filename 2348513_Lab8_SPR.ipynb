{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deyonrose/Speech_Processing/blob/main/2348513_Lab8_SPR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**implement the Viterbi Algorithm**"
      ],
      "metadata": {
        "id": "NIbCD8CcRjc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Viterbi Algorithm is a computational technique designed to determine the most likely sequence of hidden states in a Hidden Markov Model (HMM) based on observed data. By systematically evaluating transition and emission probabilities for each time step, the algorithm identifies the optimal sequence of states through a process of dynamic programming and backtracking. This makes it a powerful tool for decoding sequences in fields such as speech recognition and bioinformatics."
      ],
      "metadata": {
        "id": "Fd9TE55HRn9s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_VteomeqQo8F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def viterbi_algorithm(observation_sequence, states, start_prob, trans_prob, emis_prob):\n",
        "    n_states = len(states)\n",
        "    n_observations = len(observation_sequence)\n",
        "\n",
        "    # Initialize the viterbi matrix and backpointer matrix\n",
        "    viterbi = np.zeros((n_states, n_observations))\n",
        "    backpointer = np.zeros((n_states, n_observations), dtype=int)\n",
        "\n",
        "    # Initialization step\n",
        "    for s in range(n_states):\n",
        "        viterbi[s, 0] = start_prob[s] * emis_prob[s, observation_sequence[0]]\n",
        "        backpointer[s, 0] = 0\n",
        "\n",
        "    # Recursion step\n",
        "    for t in range(1, n_observations):\n",
        "        for s in range(n_states):\n",
        "            probabilities = [viterbi[s_prev, t - 1] * trans_prob[s_prev, s] * emis_prob[s, observation_sequence[t]] for s_prev in range(n_states)]\n",
        "            viterbi[s, t] = max(probabilities)\n",
        "            backpointer[s, t] = np.argmax(probabilities)\n",
        "\n",
        "    # Termination step\n",
        "    last_state = np.argmax(viterbi[:, -1])\n",
        "    most_likely_sequence = [last_state]\n",
        "    probability_of_sequence = viterbi[last_state, -1]\n",
        "\n",
        "    # Traceback step\n",
        "    for t in range(n_observations - 1, 0, -1):\n",
        "        last_state = backpointer[last_state, t]\n",
        "        most_likely_sequence.insert(0, last_state)\n",
        "\n",
        "    return most_likely_sequence, probability_of_sequence\n",
        "\n",
        "# Define the parameters of the HMM\n",
        "states = [\"/h/\", \"/e/\", \"/l/\", \"/o/\"]\n",
        "observations = [\"O1\", \"O2\", \"O3\", \"O4\"]\n",
        "observation_map = {\"O1\": 0, \"O2\": 1, \"O3\": 2, \"O4\": 3}\n",
        "\n",
        "start_probabilities = np.array([1.0, 0.0, 0.0, 0.0])\n",
        "transition_probabilities = np.array([\n",
        "    [0.0, 0.7, 0.3, 0.0],\n",
        "    [0.0, 0.2, 0.6, 0.2],\n",
        "    [0.0, 0.0, 0.3, 0.7],\n",
        "    [0.0, 0.0, 0.1, 0.9]\n",
        "])\n",
        "emission_probabilities = np.array([\n",
        "    [0.6, 0.2, 0.1, 0.1],\n",
        "    [0.1, 0.7, 0.1, 0.1],\n",
        "    [0.1, 0.1, 0.6, 0.2],\n",
        "    [0.2, 0.1, 0.2, 0.5]\n",
        "])\n",
        "\n",
        "# Observation sequence captured by the system\n",
        "observation_sequence = [observation_map[o] for o in [\"O1\", \"O2\", \"O3\", \"O4\"]]\n",
        "\n",
        "# Run the Viterbi algorithm\n",
        "most_likely_sequence, sequence_probability = viterbi_algorithm(\n",
        "    observation_sequence, states, start_probabilities, transition_probabilities, emission_probabilities\n",
        ")\n",
        "\n",
        "# Convert state indices to phonemes\n",
        "most_likely_states = [states[state] for state in most_likely_sequence]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the results\n",
        "print(\"Most likely sequence of phoneme states:\", most_likely_states)\n",
        "print(\"Probability of the most likely sequence:\", sequence_probability)\n",
        "\n",
        "# Inference\n",
        "print(\"The decoded phoneme sequence matches the expected sequence (/h/, /e/, /l/, /o/) for the observation [O1, O2, O3, O4].\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I7kEaSkSqdI",
        "outputId": "d27195d5-fabd-4143-fa51-c7bc5a4402ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most likely sequence of phoneme states: ['/h/', '/e/', '/l/', '/o/']\n",
            "Probability of the most likely sequence: 0.03704399999999999\n",
            "The decoded phoneme sequence matches the expected sequence (/h/, /e/, /l/, /o/) for the observation [O1, O2, O3, O4].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference"
      ],
      "metadata": {
        "id": "7Qqc3nlGSCBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The Viterbi algorithm has accurately identified the phoneme sequence (/h/, /e/, /l/, /o/) corresponding to the observation [O1, O2, O3, O4]. This result demonstrates the model's effectiveness in decoding the expected word \"hello\" with a sequence probability of 0.03704."
      ],
      "metadata": {
        "id": "Y1j23D6kSDjn"
      }
    }
  ]
}